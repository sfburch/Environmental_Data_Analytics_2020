---
title: "14: Time Series"
author: "Environmental Data Analytics | Kateri Salk"
date: "Spring 2020"
output: pdf_document
geometry: margin=2.54cm
editor_options: 
  chunk_output_type: console
---

## Objectives
1. Discuss the purpose and application of time series analysis for environmental data
2. Choose appropriate time series analyses for trend detection 
3. Address the influence of seasonality on time series analysis
4. Interpret and communicate results of time series analyses 

## Set up
```{r, message = FALSE}
getwd()
library(tidyverse)
library(lubridate)
install.packages("trend") 
library(trend)
install.packages("zoo")
library(zoo)
library(ggplot2)

# Set theme she likes to use theme classic, what is the point of themes...base size of the font, 14 gives more readability to the text, helpful for visualizations
mytheme <- theme_classic(base_size = 14) +
  theme(axis.text = element_text(color = "black"), 
        legend.position = "top")
theme_set(mytheme)

EnoDischarge <- read.csv("./Data/Processed/USGS_Site02085000_Flow_Processed.csv")
EnoDischarge$datetime <- as.Date(EnoDischarge$datetime, format = "%m/%d/%y") #pay attention to dates

NCAir <- read.csv("./Data/Processed/EPAair_O3_PM25_NC1819_Processed.csv")
NCAir$Date <- as.Date(NCAir$Date, format = "%Y-%m-%d")
```


## Time Series Analysis

Time series are a special class of dataset, where a response variable is tracked over time. The frequency of measurement and the timespan of the dataset can vary widely. At its most simple, a time series model includes an explanatory time component and a response variable. Mixed models can include additional explanatory variables (check out the `nlme` and `lme4` R packages). We will cover a few simple applications of time series analysis in these lessons, with references for how to take analyses further.

### Opportunities

Analysis of time series presents several opportunities. For environmental data, some of the most common questions we can answer with time series modeling are:

* Has there been an increasing or decreasing **trend** in the response variable over time?
* Can we **forecast** conditions in the future?

In most env data, there are some common qs (the two above). Think about trends over time and can we forecast conditions into the future (we won't do as much forecasting today?).

### Challenges

Time series datasets come with several caveats, which need to be addressed in order to effectively model the system. A few common challenges that arise (and can occur together within a single dataset) are: 

* **Autocorrelation**: Data points are not independent from one another (i.e., the measurement at a given time point is dependent on previous time point(s)) temperature today may be good predictor of temp tmw; has important effect on how we use time series data
* **Data gaps**: Data are not collected at regular intervals, necessitating *interpolation* between measurements. We may not sample temp on specfific/given day so think about adjusting data to sample on regular intervals
* **Seasonality**: Cyclic patterns in variables occur at regular intervals, impeding clear interpretation of a monotonic (unidirectional) trend.
* **Heteroscedasticity**: The variance of the time series is not constant over time (getting more or less variable over time which can present a problem), more for forecasting
* **Covariance**: the covariance of the time series is not constant over time (neither increasing or decreasing?), more for forecasting

### Example dataset: Eno River Discharge

River discharge is measured daily at the Eno River gage station. Since we are working with one location measured over time, this will make a great example dataset for time series analysis. 

Let's look at what the dataset contains for mean daily discharge.

```{r}
ggplot(EnoDischarge, aes(x = datetime, y = discharge.mean)) +
  geom_line() +
  labs(x = "", y = expression("Discharge (ft"^3*"/s)")) #error: must be a finite number; many outliers
```

Notice there are missing data from 1971 to 1985. Gaps this large are generally an issue for time series analysis, as we don't have a continuous record of data or a good way to characterize any variability that happened over those years. We will illustrate a few workarounds to address these issues. 

Let's start by removing the NAs and splitting the dataset into the early and late years. 

```{r}
EnoDischarge.complete <- EnoDischarge %>%
  drop_na(discharge.mean)

EnoDischarge.early <- EnoDischarge.complete %>%
  filter(datetime < as.Date("1985-01-01")) #this will fall into the early data set

EnoDischarge.late <- EnoDischarge.complete %>%
  filter(datetime > as.Date("1985-01-01"))
```

## Decomposing a time series dataset

A given time series can be made up of several component series: 

1. A **seasonal** component, which repeats over a fixed known period (e.g., seasons of the year, months, days of the week (traffic patterns), hour of the day (oxygen ocncentrations in a stream)) we'll work mainly with seasonal or monthly trends
2. A **trend** component, which quantifies the upward or downward progression over time. The trend component of a time series does not have to be monotonic.
3. An **error** or **random** component, which makes up the remainder of the time series after other components have been accounted for. This component reflects the noise in the dataset. 
4. (optional) A **cyclical** component, which repeats over periods greater than the seasonal component. A good example of this is El Ni√±o Southern Oscillation (ENSO) cycles, which occur over a period of 2-8 years.

We will decompose the EnoDischarge.late data frame (after year 1984) for illustrative purposes today. It is possible to run time series analysis on detrended data by subtracting the trend component from the data. However, detrending must be done carefully, as many environmental data are bounded by zero but are not treated as such in a decomposition. If you plan to use decomposition to detrend your data, please consult time series analysis guides before proceeding. You can de-trend a time series analysis, like how might something look if climate change was not a factor.

We first need to turn the discharge data into a time series object in R. This is done using the `ts` function. Notice we can only specify one column of data and need to specify the period at which the data are sampled. The resulting time series object cannot be viewed like a regular data frame.

Note: time series objects must be equispaced (have a to have a number for eveyr day, or something for every month; time series will not preserve dates??). In our case, we have daily data with no NAs in the data frame, so we don't need to worry about this. We will cover how to address data that are not equispaced later in the lesson.

```{r}
EnoDischarge.late_ts <- ts(EnoDischarge.late[[8]], frequency = 365) #ts is time series function; 8th column is dischargemean (what we're focusing on); we can subset, freqeuncy of 365 will generate daily data; what does the 8 mean?? 8th column of data set...shows up in our value section can't click on it (27k pieces of data, same number of rows but why? storing them in order) can't inspect 
```

The `stl` function decomposes the time series object into its component parts. We must specify that the window for seasonal extraction is either "periodic" or a specific number of at least 7. The decomposition proceeds through a loess (locally estimated scatterplot smoothing) function. loess extracts certain components

```{r}
?stl #help bar, not helpful...
# Generate the decomposition, potential for seasonality and trend over time
EnoDischarge.late_Decomposed <- stl(EnoDischarge.late_ts, s.window = "periodic") #specify window, periodic finds the seasonal data

# Visualize the decomposed series. trend is moving average (periods of time it's higher or lower) high point of 12-13 is not consistent in trend; time is over 35 years; no info about time represents; the boxes represent relative differences in axes relative size of the bar; data = actual measurements (seasonal + trend + ?); remainder - seasonal and trend only make up small amount of the data sclaes are small relative to data so most of data falls under remainder (what the hell is remainder?); not a strong seasonal trend
plot(EnoDischarge.late_Decomposed)

# We can extract the components and turn them into data frames; grab solumns 1-3
EnoDischarge.late_Components <- as.data.frame(EnoDischarge.late_Decomposed$time.series[,1:3])

EnoDischarge.late_Components <- mutate(EnoDischarge.late_Components,
                      Observed = EnoDischarge.late$discharge.mean,     
                      Date = EnoDischarge.late$datetime) #created two new columns one called observed and date, can mutate on whole column bc same number of rows???

# Visualize how the trend maps onto the data, observed discharge by date then another geom line to specify tren compnentn to visualize how it measure up with trend over time, pink line shows trends i.e. when discharge is high
ggplot(EnoDischarge.late_Components) +
  geom_line(aes(y = Observed, x = Date),  size = 0.5) +
  geom_line(aes(y = trend, x = Date), color = "#c13d75ff", size = 1) +
  geom_hline(yintercept = 0, lty = 2) + #specifying 0 discharge
  ylab(expression("Discharge (ft"^3*"/s)"))

# Visualize how the seasonal cycle maps onto the data, plot seasonal instead of trend, seasonal dips down below zero consisenyl each year, doesn't care about it - shows negative values so interpret it carefully
ggplot(EnoDischarge.late_Components) +
  geom_line(aes(y = Observed, x = Date),  size = 0.25) +
  geom_line(aes(y = seasonal, x = Date), color = "#c13d75ff") +
  geom_hline(yintercept = 0, lty = 2) +
  ylab(expression("Discharge (ft"^3*"/s)"))

```

Note that the decomposition can yield negative values when we apply a seasonal adjustment or a trend adjustment to the data. The decomposition is not constrained by a lower bound of zero as discharge is in real life. Make sure to interpret with caution!


## Trend analysis

Two types of trends may be present in our time series dataset: **monotonic** or **step**. Monotonic trends are a gradual shift over time that is consistent in direction, for example in response to land use change. Step trends are a distinct shift at a given time point, for example in response to a policy being enacted. Monotonic could be land use change bc happening more gradually

### Step trend analysis

Step trend analysis works well for upstream/downstream and before/after study design. We will not delve into each of these methods during class, but specific tests are listed below for future reference. Might be assoicated with policy change and we saw total response change in X. up and downstream river data 

Note: ALWAYS look into the assumptions of a given test to ensure it matches with your data and with your research question.

* **Change point detection**, e.g., `pettitt.test` (package: trend), `breakpoints` (package: strucchange), `chngpt.test` (package: chngpt), multiple tests in package: changepoint
* **t-test (paired or unpaired)**
* **Kruskal-Wallis test**: non-parametric version of t-test
* **ANCOVA**, analysis of covariance

#### Example: step trend analysis
Let's say we wanted to know whether discharge was higher in the early period or the late period. Perhaps there was a change in the methodology of streamflow measurement between the two periods that caused a differene in the magnitude of measured discharge?

```{r}
EnoDischarge.early.subsample <- sample_n(EnoDischarge.early, 5000)
EnoDischarge.late.subsample <- sample_n(EnoDischarge.late, 5000)

shapiro.test(EnoDischarge.early.subsample$discharge.mean)
shapiro.test(EnoDischarge.late.subsample$discharge.mean)

var.test(EnoDischarge.early$discharge.mean, EnoDischarge.late$discharge.mean)

wilcox.test(EnoDischarge.early$discharge.mean, EnoDischarge.late$discharge.mean)
```

How might you interpret the results of this test, and how might you represent them graphically? 

>Mean daily discharge in Eno River from 1927-1971 was significantly higher than ,ean dialydischarge from 1985-2019 (Wilcox test, W = 1.17*10^, p < 0.0001); could use density or box plot to visualize

### Monotonic trend analysis

In general, detecting a monotonic trend requires a long sequence of data with few gaps. If we are working with monthly data, a time series of at least five years is recommended. Gaps can be accounted for, but a gap that makes up more than 1/3 of the sampling period is generally considered the threshold for considering a gap to be too long (a step trend analysis might be better in this situation). 

Adjusting the data may be necessary to fulfill the assumptions of a trend test. These adjustments include **aggregation**, **subsampling**, and **interpolation**. What do each of these mean, and why might we want to use them?

> aggregation: Taking data points and making one data point from them. Could aggregate inot longer timespan; good backdoor way to get around data gaps 28 days of a month you could take summary of it to get general sense??

> subsampling: use the least out of time series analyses; could take first day of every month

> interpolation: predicting data gaps - more below:

Common interpolation methods: options to go with one of these three, another method isn't on here

* **Piecewise constant**: also known as a "nearest neighbor" approach. Any missing data are assumed to be equal to the measurement made nearest to that date (could be earlier or later).
* **Linear**: could be thought of as a "connect the dots" approach. Any missing data are assumed to fall between the previous and next measurement, with a straight line drawn between the known points determining the values of the interpolated data on any given date. whatevr our data value would be would between closest points?? the missing day would be mean of previous day and the next day
* **Spline**: similar to a linear interpolation except that a quadratic function is used to interpolate rather than drawing a straight line. smooth line

#### Example: interpolation

The Eno River discharge data doesn't have any short periods of missing data, so interpolation would not be a good choice for that dataset. We will illustrate a linear interpolation of the NC Air quality dataset below. 

In this case, several sites have a lot of missing data, and several sites monitor most days with few missing data points. 
```{r}
NCOzone <-
ggplot(NCAir, aes(x = Date, y = Ozone)) +
  geom_point() +
  facet_wrap(vars(Site.Name))
print(NCOzone) #13 sites, 2 yrs of data, some have gaps and some are ocntinuous, should we interpolate across seasonal gaps? NO, bc many of them see this time of yr we see our low pts so if we did it across months, we'd probably end up with a straight line; would be good if we had a shorter gap

NCPM2.5 <-
ggplot(NCAir, aes(x = Date, y = PM2.5)) +
  geom_point() +
  facet_wrap(vars(Site.Name))
print(NCPM2.5) #same story as above, we see less sseaonality and season gaps, and some data has not been reported for the year

summary(NCAir$Site.Name) #will give me how many samples (how many rows) in data frame; should have 730 (bc two yrs (365 + 365) samples but some have fewer; garinger hs is good example bc we have 8 missing days to acct for

NCAir.Garinger <- NCAir %>%
  filter(Site.Name == "Garinger High School")

#make gg plot
GaringerOzone <- ggplot(NCAir.Garinger, aes(x = Date, y = Ozone)) +
  geom_point() 
print(GaringerOzone) #still missing data so do the below, we know there's seasonality

# na.approx function fills in NAs with a linear interpolation (if we have missing day it'll take mean of two days and draw a straight line btwn them)
# Spline interpolation can also be specified as an alternative
# Piecewise constant interpolation can be done with na.aggregate
NCAir.Garinger$Ozone <- na.approx(NCAir.Garinger$Ozone) #do na approx on both columns, NA values are included in data frame and this gets rid of them for this column
NCAir.Garinger$PM2.5 <- na.approx(NCAir.Garinger$PM2.5)

GaringerOzone.interpolated <-
ggplot(NCAir.Garinger, aes(x = Date, y = Ozone)) +
  geom_point() 
print(GaringerOzone.interpolated) #this filled in some gaps (look between the two ggplots we created)
```

### Monotonic trend analysis, continued

Specific tests for monotonic trend analysis are listed below, with assumptions and tips: 

* **linear regression**: no seasonality, fits the assumptions of a parametric test. Function: `lm`; sub out time for what????; works when we have equi spaced data and data is unidirectional (yrly avergaes and totals of something; just shows for ex if something is increasign over time)
* **Mann-Kendall**: no seasonality, non-parametric, no temporal autocorrelation (temp autocorr: pts that are more related to each other, further away you get in time the worse?), missing data allowed. Function: `mk.test` (package: trend)
* **modified Mann-Kendall**: no seasonality, non-parametric, accounts for temporal autocorrelation, missing data allowed. Function: `mmky` and `mmkh` (package: modifiedmk)
* **Seasonal Mann-Kendall**: seasonality, non-parametric, no temporal autocorelation, identical distribution. Function: `smk.test` (package: trend) allows for seasonality in data, have to be identically distributed

The packages trend, Kendall, and modifiedmk also include other modifications to monotonic trend tests. Look into the documentation for these packages if you are applying a special case.

If covariates (another predictor variable) are included in the dataset, additional tests are recommended. A great resource for trend testing for water quality monitoring, which includes guidance on these cases, has been prepared by the Environmental Protection Agency: https://www.epa.gov/sites/production/files/2016-05/documents/tech_notes_6_dec2013_trend.pdf. This would likely be useful for other types of environmental data too. 

#### Example: monotonic trend analysis

Remember that we noticed in the decomposition that the Eno River discharge data has a seasonal cycle (despite high random variability). We might be interested in knowing how (if) discharge has changed over the course of measurement while incorporating the seasonal component. In this case, we will use a Seasonal Mann-Kendall test to figure out whether a monotonic trend exists. We will use the late dataset again.

The Seasonal Mann-Kendall assumes no temporal autocorrelation, but we know that daily data is prone to temporal autocorrelation. In this case, we may want to collapse our data down into monthly data so that we can (1) reduce temporal autocorrelation and (2) break down the potential seasonal trend into more interpretable components. 

We will calculate the mean monthly discharge for this dataset, rather than calculating the total monthly discharge or subsampling a given day in each month. Why did we make this decision? collapse varibaility and come up with one value by aggreg ratehr than subsampling????

```{r}
EnoDischarge.late.monthly <- EnoDischarge.late %>%
  mutate(Year = year(datetime), 
         Month = month(datetime)) %>%
  group_by(Year, Month) %>%
  summarise(Discharge = mean(discharge.mean)) #why mean instead of median bc it will account for variability, sum could be good too (months won't have even days in them, so feb wuld be lowest value)? things at middle of data set, now we only have 3 columns in data but we're missing what? the actual day (just yr and month is not enough)
  
#make new column called date, created this as class of date, then paste whateevr info we have from year, then month, then value of 1 (day), separte each number with dash, then format it), but don't use in time series analysis???? don't use dat column in the analyssi bc it's asuming it's first of month but we're just using an avg for the month
EnoDischarge.late.monthly$Date <- as.Date(paste(EnoDischarge.late.monthly$Year, 
                                                EnoDischarge.late.monthly$Month, 
                                                1, sep="-"), 
                                          format = "%Y-%m-%d")

# Generate time series (smk.test needs ts, not data.frame) call it something new, ts function to create times seires; discharge column; specify a frequency 12 months in the yr and freq of that seasonality should come back aorund eevry 12 times in the data frame; starting in 1985 the 10th month and eneding in 12th month in 2019, include first day of month; ts won't contain any of that info so have to include it here
EnoDischarge.late.monthly.ts <- ts(EnoDischarge.late.monthly$Discharge, frequency = 12, 
                        start = c(1985, 10, 1), end = c(2019, 12, 1))

# Run SMK test, run it on ts object 
EnoDischarge.late.trend <- smk.test(EnoDischarge.late.monthly.ts) 

# Inspect results
EnoDischarge.late.trend #call up
summary(EnoDischarge.late.trend) #more detailed look, null is that there is significant trend over time, computes s and variance of s which is just diff between time steps don't need to interpet though...all p avlaues above.05 so no, not significant

EnoDischarge.monthly <-
ggplot(EnoDischarge.late.monthly, aes(x = Date, y = Discharge)) +
  geom_point() +
  geom_line()
print(EnoDischarge.monthly) #can't discern trend over time through this bc there's so much variability, no signficiant trend in this data
```

What would we conclude based on these findings? 

>can't discern trend over time through this bc there's so much variability, no signficiant trend in this data. subsampling may prove better?? check... aggregate by season potentially? overlay with something else to understand outside factors; graph moving average? would be easier to look at; report z and p?? both don't have degrees of freedom for this reporting
 
If a significant trend was present, we could compute a **Sen's Slope** to quantify that trend (`sens.slope` function in the trend package).


## Forecasting with Autoregressive and Moving Average Models (ARMA)

We might be interested in characterizing a time series in order to understand what happened in the past and to effectively forecast into the future. Two common models that can approximate time series are **autoregressive** and **moving average** models. To classify these models, we use the  **ACF (autocorrelation function)** and the **PACF (partial autocorrelation function)**, which correspond to the autocorrelation of a series and the correlation of the residuals, respectively. 

**Autoregressive** models operate under the framework that a given measurements is correlated with  previous measurements. For example, an AR1 formulation dictates that a measurement is dependent on the previous measurement, and the value can be predicted by quantifying the lag. 

**Moving average** models operate under the framework that the covariance between a measurement and the previous measurement is zero. While AR models use past forecast *values* to predict future values, MA models use past forecast *errors* to predict future values.

Here are some great resources for examining ACF and PACF lags under different formulations of AR and MA models. 
https://nwfsc-timeseries.github.io/atsa-labs/sec-tslab-autoregressive-ar-models.html
https://nwfsc-timeseries.github.io/atsa-labs/sec-tslab-moving-average-ma-models.html

ARMA models require stationary data. This means that there is no monotonic trend over time and there is also equal variance and covariance across the time series. The function `adf.test` will determine whether our data are stationary. The null hypothesis is that the data are not stationary, so we infer that the data are stationary if the p-value is < 0.05.

While some processes might be easy to identify, it is often complicated to predict the order of AR and MA processes when the operate in the same dataset. To get around this issue, it is often necessary to run multiple potential formulations of the model and see which one results in the most parsimonious fit using AIC. The function `auto.arima` does this automatically.

